{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus('./data/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Text8Corpus at 0x7f365756b350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:13:55,255 : INFO : collecting all words and their counts\n",
      "2017-03-02 21:13:55,258 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-02 21:14:00,232 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-03-02 21:14:00,233 : INFO : Loading a fresh vocabulary\n",
      "2017-03-02 21:14:00,614 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-03-02 21:14:00,615 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-03-02 21:14:00,854 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-03-02 21:14:00,882 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-03-02 21:14:00,883 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-03-02 21:14:00,884 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2017-03-02 21:14:01,115 : INFO : resetting layer weights\n",
      "2017-03-02 21:14:01,776 : INFO : training model with 4 workers on 71290 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-03-02 21:14:01,776 : INFO : expecting 1701 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-02 21:14:02,788 : INFO : PROGRESS: at 1.54% examples, 952749 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-02 21:14:03,792 : INFO : PROGRESS: at 3.19% examples, 983792 words/s, in_qsize 7, out_qsize 2\n",
      "2017-03-02 21:14:04,802 : INFO : PROGRESS: at 4.84% examples, 999169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:05,803 : INFO : PROGRESS: at 6.43% examples, 999891 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:06,811 : INFO : PROGRESS: at 8.04% examples, 1001185 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:07,815 : INFO : PROGRESS: at 9.69% examples, 1005747 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:08,817 : INFO : PROGRESS: at 11.28% examples, 1004718 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:09,831 : INFO : PROGRESS: at 12.83% examples, 999896 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:14:10,830 : INFO : PROGRESS: at 14.47% examples, 1002625 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:11,834 : INFO : PROGRESS: at 16.16% examples, 1005415 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:12,852 : INFO : PROGRESS: at 17.75% examples, 1003204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:13,867 : INFO : PROGRESS: at 19.38% examples, 1002561 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:14,872 : INFO : PROGRESS: at 20.98% examples, 1001627 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:15,874 : INFO : PROGRESS: at 22.56% examples, 1000099 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:14:16,887 : INFO : PROGRESS: at 24.17% examples, 1000254 words/s, in_qsize 4, out_qsize 1\n",
      "2017-03-02 21:14:17,890 : INFO : PROGRESS: at 25.74% examples, 998814 words/s, in_qsize 5, out_qsize 0\n",
      "2017-03-02 21:14:18,903 : INFO : PROGRESS: at 27.34% examples, 998817 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:14:19,914 : INFO : PROGRESS: at 28.96% examples, 999236 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:20,920 : INFO : PROGRESS: at 30.55% examples, 998752 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:14:21,927 : INFO : PROGRESS: at 32.24% examples, 1001682 words/s, in_qsize 5, out_qsize 1\n",
      "2017-03-02 21:14:22,932 : INFO : PROGRESS: at 33.84% examples, 1001442 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:23,943 : INFO : PROGRESS: at 35.50% examples, 1002167 words/s, in_qsize 6, out_qsize 2\n",
      "2017-03-02 21:14:24,939 : INFO : PROGRESS: at 37.15% examples, 1003477 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:25,944 : INFO : PROGRESS: at 38.74% examples, 1002651 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:26,949 : INFO : PROGRESS: at 40.32% examples, 1001672 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:27,950 : INFO : PROGRESS: at 41.98% examples, 1002523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:28,961 : INFO : PROGRESS: at 43.50% examples, 1000615 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:29,953 : INFO : PROGRESS: at 45.04% examples, 999387 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-02 21:14:30,962 : INFO : PROGRESS: at 46.57% examples, 998088 words/s, in_qsize 5, out_qsize 1\n",
      "2017-03-02 21:14:31,967 : INFO : PROGRESS: at 48.12% examples, 997159 words/s, in_qsize 3, out_qsize 1\n",
      "2017-03-02 21:14:32,977 : INFO : PROGRESS: at 49.72% examples, 997067 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-02 21:14:33,979 : INFO : PROGRESS: at 51.32% examples, 997240 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:34,981 : INFO : PROGRESS: at 52.87% examples, 996483 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:35,983 : INFO : PROGRESS: at 54.44% examples, 995947 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:37,002 : INFO : PROGRESS: at 56.07% examples, 996028 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:14:37,995 : INFO : PROGRESS: at 57.66% examples, 995767 words/s, in_qsize 5, out_qsize 1\n",
      "2017-03-02 21:14:39,016 : INFO : PROGRESS: at 59.24% examples, 994903 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:40,022 : INFO : PROGRESS: at 60.85% examples, 994910 words/s, in_qsize 4, out_qsize 0\n",
      "2017-03-02 21:14:41,025 : INFO : PROGRESS: at 62.42% examples, 994285 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-02 21:14:42,042 : INFO : PROGRESS: at 64.07% examples, 994662 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:14:43,054 : INFO : PROGRESS: at 65.61% examples, 993861 words/s, in_qsize 5, out_qsize 2\n",
      "2017-03-02 21:14:44,059 : INFO : PROGRESS: at 67.21% examples, 994208 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:45,061 : INFO : PROGRESS: at 68.79% examples, 994161 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:46,062 : INFO : PROGRESS: at 70.37% examples, 994053 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:47,069 : INFO : PROGRESS: at 71.92% examples, 993651 words/s, in_qsize 6, out_qsize 2\n",
      "2017-03-02 21:14:48,075 : INFO : PROGRESS: at 73.59% examples, 994644 words/s, in_qsize 6, out_qsize 2\n",
      "2017-03-02 21:14:49,070 : INFO : PROGRESS: at 75.18% examples, 994433 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:50,071 : INFO : PROGRESS: at 76.72% examples, 993587 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:51,084 : INFO : PROGRESS: at 78.33% examples, 993585 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:52,107 : INFO : PROGRESS: at 79.87% examples, 992430 words/s, in_qsize 8, out_qsize 2\n",
      "2017-03-02 21:14:53,111 : INFO : PROGRESS: at 81.50% examples, 992757 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:14:54,119 : INFO : PROGRESS: at 83.16% examples, 993342 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:14:55,123 : INFO : PROGRESS: at 84.75% examples, 993373 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:56,123 : INFO : PROGRESS: at 86.40% examples, 994223 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:14:57,130 : INFO : PROGRESS: at 88.02% examples, 994597 words/s, in_qsize 6, out_qsize 0\n",
      "2017-03-02 21:14:58,132 : INFO : PROGRESS: at 89.62% examples, 994733 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:14:59,143 : INFO : PROGRESS: at 91.18% examples, 994455 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:15:00,142 : INFO : PROGRESS: at 92.77% examples, 994370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:15:01,144 : INFO : PROGRESS: at 94.36% examples, 994333 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:15:02,153 : INFO : PROGRESS: at 96.00% examples, 994492 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:15:03,158 : INFO : PROGRESS: at 97.58% examples, 994217 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:15:04,160 : INFO : PROGRESS: at 99.20% examples, 994489 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:15:04,626 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-02 21:15:04,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-02 21:15:04,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-02 21:15:04,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-02 21:15:04,636 : INFO : training on 85026035 raw words (62531767 effective words) took 62.9s, 994817 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:15:51,804 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.679977536201477)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.679977536201477), (u'prince', 0.6265594363212585)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.7342628240585327),\n",
       " (u'girl', 0.6680829524993896),\n",
       " (u'creature', 0.6141831874847412),\n",
       " (u'stranger', 0.598820686340332),\n",
       " (u'boy', 0.5740600228309631),\n",
       " (u'totoro', 0.560284435749054),\n",
       " (u'god', 0.5485066771507263),\n",
       " (u'person', 0.5453802347183228),\n",
       " (u'thief', 0.5392208099365234),\n",
       " (u'evil', 0.5389797687530518)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:20:52,498 : INFO : saving Word2Vec object under text8.model.bin, separately None\n",
      "2017-03-02 21:20:52,499 : INFO : not storing attribute syn0norm\n",
      "2017-03-02 21:20:52,500 : INFO : not storing attribute cum_table\n",
      "2017-03-02 21:20:53,211 : INFO : saved text8.model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save('text8.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'save_word2vec_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f1fa7ed1100e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text.model.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'save_word2vec_format'"
     ]
    }
   ],
   "source": [
    "# model.save_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:21:03,281 : INFO : loading Word2Vec object from text8.model.bin\n",
      "2017-03-02 21:21:03,569 : INFO : loading wv recursively from text8.model.bin.wv.* with mmap=None\n",
      "2017-03-02 21:21:03,570 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-02 21:21:03,570 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-02 21:21:03,571 : INFO : loaded text8.model.bin\n"
     ]
    }
   ],
   "source": [
    "# model1 = word2vec.Word2Vec.load_word2vec_format('text8.model.bin', binary=True)\n",
    "model1 = word2vec.Word2Vec.load('text8.model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:19:33,831 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'mother', 0.8032259345054626),\n",
       " (u'wife', 0.7479128837585449),\n",
       " (u'grandmother', 0.7439329624176025)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar(['girl', 'father'], ['boy'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'mother', 0.8032259345054626),\n",
       " (u'wife', 0.7479128837585449),\n",
       " (u'grandmother', 0.7439329624176025)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['girl', 'father'], ['boy'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'he' is to 'is' as 'she' is to 'remains'\n",
      "'big' is to 'bigger' as 'bad' is to 'worse'\n",
      "'going' is to 'went' as 'being' is to 'was'\n"
     ]
    }
   ],
   "source": [
    "more_examples = [\"he is she\", \"big bigger bad\", \"going went being\"]\n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print \"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model_org = word2vec.Word2Vec.load_word2vec_format('vectors.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75201175555969291"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99474508, -0.56053334, -1.19015479,  0.7491926 , -0.86471349,\n",
       "        1.91366148,  1.5654043 ,  0.96848601,  1.58055401, -0.80185962,\n",
       "       -0.19310495, -0.42493778,  1.00711811, -0.9489342 ,  1.30079377,\n",
       "        0.41287372,  0.93818945, -0.89474839, -1.31119096,  1.37997258,\n",
       "        2.92478967,  0.81668627,  1.79179692,  0.95069277,  0.95391285,\n",
       "        0.54854596,  0.03385108, -1.07700157, -0.04401682, -0.99985522,\n",
       "       -1.49530697, -1.73004293,  0.52372754,  0.95713454,  0.20329179,\n",
       "        0.10182901,  6.07298183, -2.7215941 , -2.70838666, -1.84513569,\n",
       "       -0.9648242 ,  0.223185  , -1.44931436, -0.27098772,  1.76999533,\n",
       "       -2.1044445 ,  2.19885707,  1.02528167, -0.24892575, -1.65061748,\n",
       "       -1.67411745, -0.43030879, -4.76971149,  0.22593763,  2.95434093,\n",
       "        0.96193582, -0.52248925, -0.17284206,  2.10687113, -0.75352907,\n",
       "       -3.4838202 , -0.63512242, -1.46311331, -0.58760852, -0.61493397,\n",
       "        0.42243025, -0.33387402, -0.22156897,  1.70457673,  2.14774346,\n",
       "        0.31320968, -1.99263227,  2.14152765,  1.20632637, -0.66853386,\n",
       "       -0.96731406,  0.23820437, -0.17667243, -0.51987827,  2.18319726,\n",
       "        0.30316401, -2.63983679, -1.87756503, -2.05690002, -2.1943028 ,\n",
       "        1.08658755,  2.84106374, -1.04560435, -2.69670367, -0.05519718,\n",
       "       -1.56544197,  0.71076047,  1.85255897, -0.44817856,  2.02908063,\n",
       "        0.41416782, -0.19564793, -1.5504235 , -0.57562113, -0.68533432], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print model['computer'].shape\n",
    "model['computer']  # raw numpy vector of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

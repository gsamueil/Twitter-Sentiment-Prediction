{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "# This file makes predictions on incoming data and perform basic analysis.\n",
    "import pandas as pd\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from load_data import Data\n",
    "from utils import load_network\n",
    "FILE_PATH = '/home/sam/Hhd/twitter_sentiment/'\n",
    "# FILE_PATH = '/home/sam/Data/twitter_sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading csv: air.csv ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @dimitrivegas: For more info... please visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@AmericanAir I don't think you do!! 90% of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@AmericanAir they said it was due to size and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon, SeaWorld, @AmericanAir Downgraded - @S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julie, @AmericanAir manager in Bozeman, MT, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  RT @dimitrivegas: For more info... please visi...\n",
       "1  @AmericanAir I don't think you do!! 90% of the...\n",
       "2  @AmericanAir they said it was due to size and ...\n",
       "3  Amazon, SeaWorld, @AmericanAir Downgraded - @S...\n",
       "4  Julie, @AmericanAir manager in Bozeman, MT, th..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"air.csv\"\n",
    "\n",
    "test_data = Data(file_name, FILE_PATH)\n",
    "test_df = test_data.csv_df(['text'])\n",
    "# make a copy of the original tweets for later use\n",
    "original_df = test_df.copy()\n",
    "# original tweets\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: pre-process changes the dataframe inplace.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT  For more info please visit this shortcut l...</td>\n",
       "      <td>[info, please, visit, shortcut, eduaubdedubu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dont think you do  of the flights Ive had w...</td>\n",
       "      <td>[dont, think, flights, ive, bags, delayed, fli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they said it was due to size and I have never...</td>\n",
       "      <td>[said, due, size, never, problem, fitting, ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SeaWorld  Downgraded   blog</td>\n",
       "      <td>[amazon, seaworld, downgraded, blog]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julie  manager in Bozeman MT thanks much for t...</td>\n",
       "      <td>[manager, mt, thanks, much, upgrade, yesterday...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  RT  For more info please visit this shortcut l...   \n",
       "1   I dont think you do  of the flights Ive had w...   \n",
       "2   they said it was due to size and I have never...   \n",
       "3               Amazon SeaWorld  Downgraded   blog     \n",
       "4  Julie  manager in Bozeman MT thanks much for t...   \n",
       "\n",
       "                                           tokenized  \n",
       "0      [info, please, visit, shortcut, eduaubdedubu]  \n",
       "1  [dont, think, flights, ive, bags, delayed, fli...  \n",
       "2  [said, due, size, never, problem, fitting, ame...  \n",
       "3               [amazon, seaworld, downgraded, blog]  \n",
       "4  [manager, mt, thanks, much, upgrade, yesterday...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process data(same as how we trained)\n",
    "test_data.pre_process(test_df) \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model tweets600.model.bin ...\n",
      "Done building.\n",
      "max sentence length is:  18\n",
      "Total 7 not in vocab.\n",
      "Done converting tweets to vec!\n",
      "Saved test to disk.\n"
     ]
    }
   ],
   "source": [
    "# then convert using word2vec\n",
    "model = test_data.build_wordvec(size=600, verbose=False)\n",
    "# take a look of the max_len of testing. although we still have to use max_len from train\n",
    "max_len_test = test_data.max_len(test_df)\n",
    "max_len_train = 19\n",
    "data = test_data.convert2vec(test_df, max_len_train, model, name='test')\n",
    "test_data.save_vec(data, name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cnn(M, D, input_var=None):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, M, D), input_var=input_var)\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=40, filter_size=(3, 3), \\\n",
    "                                         nonlinearity=lasagne.nonlinearities.rectify, \\\n",
    "                                         W=lasagne.init.GlorotUniform(), pad=0, stride=(1, 1), \\\n",
    "                                         untie_biases=True)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=50, filter_size=(3, 3), \\\n",
    "                                         nonlinearity=lasagne.nonlinearities.rectify, pad=0, \\\n",
    "                                         stride=(1, 1), untie_biases=True)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, p=0.5), num_units=6000, \\\n",
    "                                        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(lasagne.layers.dropout(network, p=0.5), num_units=3,  \\\n",
    "                                        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return network\n",
    "\n",
    "def make_prediction(data):\n",
    "    \n",
    "    N, M, D = data.shape\n",
    "    print \"N, M, D:\", N, M, D\n",
    "    data = data.reshape(-1, 1, M, D).astype(theano.config.floatX) # theano needs this way\n",
    "    \n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    network = cnn(M, D, input_var)\n",
    "    \n",
    "    # now load model and do predictions\n",
    "    saved_params = load_network(FILE_PATH, \"cnn.npz\")\n",
    "    lasagne.layers.set_all_param_values(network, saved_params)\n",
    "    \n",
    "    # define prediction function\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    predict_label = T.argmax(test_prediction,axis=1)\n",
    "    test_fn = theano.function([input_var], predict_label)\n",
    "    \n",
    "    test_pred = test_fn(data) + 1\n",
    "    \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N, M, D: 1000 19 600\n"
     ]
    }
   ],
   "source": [
    "# geet predictions\n",
    "test_predictions = make_prediction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "Done converting categorical to numeric, this changes df.\n",
      "[ 'RT @dimitrivegas: For more info... please visit this shortcut link! <ed><U+00A0><U+00BD><ed><U+00B8><U+0085> @AmericanAir https://t.co/M8whrVEKMc'\n",
      " 'neutral']\n",
      "[ \"@AmericanAir I don't think you do!! 90% of the flights I've had with you my bags delayed, flights cancelled, bookings cancelled etc.\"\n",
      " 'negative']\n",
      "[ '@AmericanAir they said it was due to size and I have never once had a problem fitting it on American flights. New airline here I come'\n",
      " 'negative']\n",
      "[ 'Amazon, SeaWorld, @AmericanAir Downgraded - @Schaeffers (blog) : https://t.co/g0QTn38IH4'\n",
      " 'neutral']\n",
      "[ 'Julie, @AmericanAir manager in Bozeman, MT, thanks much for the upgrade after yesterday. Glad to have met you and wish you the best!'\n",
      " 'positive']\n",
      "[ 'Is San Francisco America\\x92s New Art Capital? In Celebrated Living for @AmericanAir #WeAreTravelMedia\\x85 https://t.co/RrulHzMw03'\n",
      " 'negative']\n",
      "[ \"@AmericanAir nah, you've had multiple chances this tear alone. Inexcusable customer service and just not giving a dam, I'm over it!!\"\n",
      " 'negative']\n",
      "[ '@AmericanAir with that totally random flight cancellation <ed><U+00A0><U+00BD><ed><U+00B1><U+0080>'\n",
      " 'negative']\n",
      "[\"@AmericanAir what isn't going on is the problem.\" 'negative']\n",
      "[ 'Baffled by @AmericanAir boarding  passengers with full knowledge that the captain is still on an inbound  flight. SMH. #FAIL'\n",
      " 'negative']\n"
     ]
    }
   ],
   "source": [
    "# write to original dataframe\n",
    "original_df['prediction'] = test_predictions\n",
    "# convert numeric prediction to categorical\n",
    "class_label = {1:'positive', 2: 'neutral', 3: 'negative'}\n",
    "original_df = test_data.num2cat(original_df, 'prediction', class_label)\n",
    "for i in range(10):\n",
    "    print original_df.values[i,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to csv\n",
    "original_df.to_csv(\"airline_predicted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

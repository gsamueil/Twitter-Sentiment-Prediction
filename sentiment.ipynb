{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os.path\n",
    "FILE_PATH = '/home/sam/Hhd/twitter_sentiment/'\n",
    "# FILE_PATH = '/home/sam/Data/twitter_sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def function to load data from json to dataframe\n",
    "def load_data(file_name):\n",
    "    print \"Loading: \" + file_name + \" ...\"\n",
    "    data_path = FILE_PATH + 'data/'\n",
    "    data_df = pd.read_json(data_path + file_name, lines=True)\n",
    "    # we only take the 'text' column\n",
    "    drop_columns = list(data_df.columns)\n",
    "    drop_columns.remove('text')\n",
    "    data_df.drop(drop_columns, axis = 1, inplace = True)\n",
    "    print \"Done loading json file to dataframe.\"\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: positive.json ...\n",
      "Done loading json file to dataframe.\n",
      "                                                text\n",
      "0          Thanks fam!!! ðŸ˜˜ðŸ˜˜ðŸ˜˜ https://t.co/sbNzT886Vs\n",
      "1  RT @PinGDP: Right back at you Ms Cassie :)\\n\\n...\n",
      "2  @notonIyou also my kinda coming out as bi :) #...\n",
      "3  RT @kevingschmidt: Thank you ktla5news for hav...\n",
      "4  RT @watchdogsgame: Celebrate our DedSec member...\n",
      "Loading: negative.json ...\n",
      "Done loading json file to dataframe.\n",
      "                                                text\n",
      "0                 I loved it https://t.co/nhezqTqCcc\n",
      "1                     I want to get my nails done :(\n",
      "2  RT @BucamanWWE: This is the last were seeing t...\n",
      "3  @TheDauntingFray // Lucky I've never done that...\n",
      "4                 @JonahBonahh I love her so much :(\n"
     ]
    }
   ],
   "source": [
    "df_pos = load_data('positive.json')\n",
    "df_pos.dropna(axis=0, inplace=True) # drop na rows\n",
    "print df_pos.head()\n",
    "df_neg = load_data('negative.json')\n",
    "df_neg.dropna(axis=0, inplace=True) # drop na rows\n",
    "print df_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stopping words:  [u'i', u'me', u'my', u'myself', u'we']\n",
      "                                                text  \\\n",
      "0                                       Thanks fam     \n",
      "1         RT  Right back at you Ms Cassie \\n\\n         \n",
      "2        also my kinda coming out as bi  issagaygirl   \n",
      "3  RT  Thank you ktlanews for having  and I on to...   \n",
      "4  RT  Celebrate our DedSec members on their birt...   \n",
      "\n",
      "                                           tokenized  \n",
      "0                                      [thanks, fam]  \n",
      "1                          [right, back, ms, cassie]  \n",
      "2             [also, kinda, coming, bi, issagaygirl]  \n",
      "3                      [thank, ktlanews, talk, link]  \n",
      "4  [celebrate, dedsec, members, birthdays, gtgt, ...  \n",
      "sample stopping words:  [u'i', u'me', u'my', u'myself', u'we']\n",
      "                                                text  \\\n",
      "0                                        I loved it    \n",
      "1                       I want to get my nails done    \n",
      "2  RT  This is the last were seeing the Hardys fo...   \n",
      "3                  Lucky Ive never done that before    \n",
      "4                                I love her so much    \n",
      "\n",
      "                                     tokenized  \n",
      "0                                      [loved]  \n",
      "1                     [want, get, nails, done]  \n",
      "2  [last, seeing, hardys, minute, impactonpop]  \n",
      "3                    [lucky, ive, never, done]  \n",
      "4                                 [love, much]  \n"
     ]
    }
   ],
   "source": [
    "def pre_process(df):\n",
    "    # remove new line char\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'\\\\n',value=r'')\n",
    "    # remove https links\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'(http|https):\\/\\/[^(\\s|\\b)]+',value=r'')\n",
    "    # remove user name\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'@\\w+',value=r'')\n",
    "    # remove non-alphabet, this includes number and punctuation\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'[^a-zA-Z\\s]',value=r'')\n",
    "    # tokenize each tweets to form sentences.\n",
    "    df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
    "    # remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    add_stop_words = ['amp', 'rt']\n",
    "    stop_words += add_stop_words\n",
    "    print \"sample stopping words: \", stop_words[:5]\n",
    "    df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    print df.head(5)\n",
    "#     return df\n",
    "pre_process(df_pos)\n",
    "pre_process(df_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now let us bring in the wordvec trained using text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-05 00:24:50,604 : INFO : collecting all words and their counts\n",
      "2017-03-05 00:24:50,605 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for tweets.model.bin ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-05 00:24:54,283 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-03-05 00:24:54,283 : INFO : Loading a fresh vocabulary\n",
      "2017-03-05 00:24:54,533 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-03-05 00:24:54,533 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-03-05 00:24:54,668 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-03-05 00:24:54,694 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-03-05 00:24:54,694 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-03-05 00:24:54,695 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2017-03-05 00:24:54,852 : INFO : resetting layer weights\n",
      "2017-03-05 00:24:55,330 : INFO : training model with 4 workers on 71290 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-03-05 00:24:55,330 : INFO : expecting 1701 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-05 00:24:56,360 : INFO : PROGRESS: at 0.96% examples, 586375 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:24:57,382 : INFO : PROGRESS: at 1.98% examples, 597829 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:24:58,404 : INFO : PROGRESS: at 3.02% examples, 611013 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:24:59,405 : INFO : PROGRESS: at 4.00% examples, 610665 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:00,416 : INFO : PROGRESS: at 5.00% examples, 613477 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:01,428 : INFO : PROGRESS: at 5.97% examples, 612930 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:02,430 : INFO : PROGRESS: at 6.97% examples, 615372 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:03,446 : INFO : PROGRESS: at 7.90% examples, 610262 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:04,464 : INFO : PROGRESS: at 8.83% examples, 606375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:05,474 : INFO : PROGRESS: at 9.82% examples, 606949 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:06,482 : INFO : PROGRESS: at 10.79% examples, 607002 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:07,498 : INFO : PROGRESS: at 11.83% examples, 609738 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:08,502 : INFO : PROGRESS: at 12.85% examples, 611959 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:09,505 : INFO : PROGRESS: at 13.86% examples, 613434 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-05 00:25:10,515 : INFO : PROGRESS: at 14.89% examples, 614954 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:11,523 : INFO : PROGRESS: at 15.94% examples, 616578 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:12,520 : INFO : PROGRESS: at 16.97% examples, 617768 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:13,525 : INFO : PROGRESS: at 17.99% examples, 618932 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:14,526 : INFO : PROGRESS: at 18.99% examples, 618899 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:15,531 : INFO : PROGRESS: at 20.00% examples, 619169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:16,533 : INFO : PROGRESS: at 21.00% examples, 619375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:17,538 : INFO : PROGRESS: at 22.02% examples, 619829 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:18,539 : INFO : PROGRESS: at 23.03% examples, 620130 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:19,542 : INFO : PROGRESS: at 24.04% examples, 620552 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:20,550 : INFO : PROGRESS: at 25.04% examples, 620681 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:21,560 : INFO : PROGRESS: at 26.06% examples, 621335 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:22,563 : INFO : PROGRESS: at 27.08% examples, 622147 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:23,569 : INFO : PROGRESS: at 28.10% examples, 622676 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:24,575 : INFO : PROGRESS: at 29.12% examples, 623232 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:25,580 : INFO : PROGRESS: at 30.15% examples, 623809 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:26,584 : INFO : PROGRESS: at 31.17% examples, 624341 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:27,589 : INFO : PROGRESS: at 32.16% examples, 624094 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:28,598 : INFO : PROGRESS: at 33.15% examples, 623729 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:29,601 : INFO : PROGRESS: at 34.13% examples, 623606 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-05 00:25:30,619 : INFO : PROGRESS: at 35.18% examples, 623898 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:31,627 : INFO : PROGRESS: at 36.21% examples, 624174 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:32,636 : INFO : PROGRESS: at 37.15% examples, 623061 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:33,638 : INFO : PROGRESS: at 38.11% examples, 622349 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:34,642 : INFO : PROGRESS: at 39.13% examples, 622596 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:35,653 : INFO : PROGRESS: at 40.15% examples, 622854 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-05 00:25:36,655 : INFO : PROGRESS: at 41.16% examples, 622860 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:37,668 : INFO : PROGRESS: at 42.20% examples, 623038 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:38,675 : INFO : PROGRESS: at 43.21% examples, 623157 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:39,678 : INFO : PROGRESS: at 44.21% examples, 623161 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:40,683 : INFO : PROGRESS: at 45.20% examples, 623132 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:41,688 : INFO : PROGRESS: at 46.21% examples, 623397 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:42,700 : INFO : PROGRESS: at 47.22% examples, 623612 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-05 00:25:43,706 : INFO : PROGRESS: at 48.21% examples, 623407 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:44,712 : INFO : PROGRESS: at 49.18% examples, 623157 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:45,715 : INFO : PROGRESS: at 50.18% examples, 623204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:46,716 : INFO : PROGRESS: at 51.16% examples, 622992 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:47,722 : INFO : PROGRESS: at 52.13% examples, 622711 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:48,731 : INFO : PROGRESS: at 53.10% examples, 622218 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:49,732 : INFO : PROGRESS: at 54.09% examples, 622166 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:50,748 : INFO : PROGRESS: at 55.07% examples, 621836 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:51,764 : INFO : PROGRESS: at 56.08% examples, 621641 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:52,775 : INFO : PROGRESS: at 57.06% examples, 621381 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-05 00:25:53,777 : INFO : PROGRESS: at 58.04% examples, 621145 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:54,782 : INFO : PROGRESS: at 59.04% examples, 621051 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:55,789 : INFO : PROGRESS: at 60.05% examples, 621115 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:56,798 : INFO : PROGRESS: at 61.06% examples, 621174 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:25:57,822 : INFO : PROGRESS: at 62.06% examples, 620861 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:58,823 : INFO : PROGRESS: at 63.08% examples, 621105 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:25:59,834 : INFO : PROGRESS: at 64.07% examples, 620988 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-05 00:26:00,834 : INFO : PROGRESS: at 65.01% examples, 620501 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:01,840 : INFO : PROGRESS: at 66.00% examples, 620544 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:02,842 : INFO : PROGRESS: at 66.96% examples, 620376 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:03,862 : INFO : PROGRESS: at 67.96% examples, 620302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:04,869 : INFO : PROGRESS: at 68.91% examples, 619913 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:05,874 : INFO : PROGRESS: at 69.82% examples, 619150 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:06,879 : INFO : PROGRESS: at 70.73% examples, 618487 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:07,882 : INFO : PROGRESS: at 71.76% examples, 618802 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:08,882 : INFO : PROGRESS: at 72.76% examples, 618911 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:09,893 : INFO : PROGRESS: at 73.77% examples, 619017 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:10,902 : INFO : PROGRESS: at 74.79% examples, 619270 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:11,907 : INFO : PROGRESS: at 75.78% examples, 618975 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:12,960 : INFO : PROGRESS: at 76.79% examples, 618776 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-05 00:26:13,954 : INFO : PROGRESS: at 77.70% examples, 618084 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:14,962 : INFO : PROGRESS: at 78.65% examples, 617712 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:15,975 : INFO : PROGRESS: at 79.69% examples, 618033 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:16,994 : INFO : PROGRESS: at 80.63% examples, 617515 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-05 00:26:18,010 : INFO : PROGRESS: at 81.60% examples, 617099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:19,016 : INFO : PROGRESS: at 82.53% examples, 616574 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:20,031 : INFO : PROGRESS: at 83.47% examples, 616126 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:21,041 : INFO : PROGRESS: at 84.42% examples, 615821 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:22,056 : INFO : PROGRESS: at 85.36% examples, 615474 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:23,058 : INFO : PROGRESS: at 86.30% examples, 615238 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:24,078 : INFO : PROGRESS: at 87.33% examples, 615459 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:25,084 : INFO : PROGRESS: at 88.31% examples, 615446 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:26,100 : INFO : PROGRESS: at 89.30% examples, 615402 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-05 00:26:27,106 : INFO : PROGRESS: at 90.31% examples, 615556 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:28,121 : INFO : PROGRESS: at 91.31% examples, 615614 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:29,130 : INFO : PROGRESS: at 92.32% examples, 615756 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:30,135 : INFO : PROGRESS: at 93.31% examples, 615745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:31,144 : INFO : PROGRESS: at 94.26% examples, 615516 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:32,147 : INFO : PROGRESS: at 95.30% examples, 615704 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-05 00:26:33,156 : INFO : PROGRESS: at 96.28% examples, 615630 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-05 00:26:34,160 : INFO : PROGRESS: at 97.31% examples, 615814 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:35,164 : INFO : PROGRESS: at 98.32% examples, 615950 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-05 00:26:36,175 : INFO : PROGRESS: at 99.33% examples, 616039 words/s, in_qsize 8, out_qsize 2\n",
      "2017-03-05 00:26:36,810 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-05 00:26:36,819 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-05 00:26:36,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-05 00:26:36,839 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-05 00:26:36,839 : INFO : training on 85026035 raw words (62534585 effective words) took 101.5s, 616061 effective words/s\n",
      "2017-03-05 00:26:36,840 : INFO : saving Word2Vec object under /home/sam/Hhd/twitter_sentiment/wordvec/tweets.model.bin, separately None\n",
      "2017-03-05 00:26:36,840 : INFO : not storing attribute syn0norm\n",
      "2017-03-05 00:26:36,841 : INFO : not storing attribute cum_table\n",
      "2017-03-05 00:26:37,317 : INFO : saved /home/sam/Hhd/twitter_sentiment/wordvec/tweets.model.bin\n",
      "2017-03-05 00:26:37,317 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building.\n"
     ]
    }
   ],
   "source": [
    "# check if the model already exists, if so load it else train the model\n",
    "def build_wordvec(sentences, model_name, size = 200):\n",
    "    model_path = FILE_PATH + 'wordvec/' + model_name\n",
    "    if os.path.isfile(model_path):\n",
    "        print \"Loading existing model {} ...\".format(model_name)\n",
    "        model = word2vec.Word2Vec.load(model_path)\n",
    "    else:\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        print \"Training for {} ...\".format(model_name)\n",
    "        model = word2vec.Word2Vec(sentences, size=size, sg=1, workers=4)\n",
    "        model.save(model_path)\n",
    "        # If youâ€™re finished training a model (=no more updates, only querying), you can do\n",
    "        model.init_sims(replace=True)\n",
    "    print \"Done building.\"\n",
    "    return model\n",
    "\n",
    "sentences = word2vec.Text8Corpus(FILE_PATH + 'data/text8')              # use text 8\n",
    "# sentences = list(df_pos['tokenized']) + list(df_neg['tokenized'])         # use just tweets itself\n",
    "vec_size = 100\n",
    "# model = load_wordvec(sentences, 'text8.model.bin', size = vec_size)\n",
    "model = build_wordvec(sentences, 'tweets.model.bin', size = vec_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform our tweets using vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length is:  17\n",
      "max sentence length is:  20\n"
     ]
    }
   ],
   "source": [
    "# first find the max length since that decides the padding\n",
    "def max_len(df):\n",
    "    df['size'] = df['tokenized'].apply(lambda x: len(x))\n",
    "    print \"max sentence length is: \", df['size'].max()\n",
    "    return df['size'].max()\n",
    "max_total = max(max_len(df_pos), max_len(df_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting tweets to vec!\n",
      "Total 71 not in vocab.\n",
      "Done converting tweets to vec!\n",
      "Total 81 not in vocab.\n"
     ]
    }
   ],
   "source": [
    "# initialize empty arry to fill with vector repsentation\n",
    "def convert2vec(df, max_total):\n",
    "    tweet_tokens = df['tokenized']\n",
    "    n = tweet_tokens.shape[0]\n",
    "    m = max_total\n",
    "    n_absent = 0\n",
    "    tweet_vecs = np.zeros((n,m,vec_size))\n",
    "    vocabs = model.wv.vocab.keys()\n",
    "    for i in range(n):\n",
    "        token_i = [x for x in tweet_tokens[i] if x in vocabs]\n",
    "        m_i = len(token_i)\n",
    "        if m_i == 0:\n",
    "            n_absent += 1\n",
    "        else:\n",
    "            diff_i = abs(m_i - m)\n",
    "            vecs_i = model[token_i]\n",
    "            tweet_vecs[i] = np.lib.pad(vecs_i, ((0,diff_i),(0,0)), 'constant', constant_values=0)\n",
    "    print \"Done converting tweets to vec!\"\n",
    "    print \"Total {} not in vocab.\".format(n_absent)\n",
    "    return tweet_vecs\n",
    "\n",
    "\n",
    "pos_vecs = convert2vec(df_pos, max_total)\n",
    "neg_vecs = convert2vec(df_neg, max_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pos to disk.\n",
      "Saved neg to disk.\n"
     ]
    }
   ],
   "source": [
    "# save tweet_vecs to disk in npy\n",
    "def save_vec(tweet_vecs, name):\n",
    "    file_name = FILE_PATH + name\n",
    "    if os.path.isfile(file_name + '.npy') and os.path.isfile(file_name + '.npz'):\n",
    "        print \"npy already exists.\"\n",
    "    else:\n",
    "        np.save(file_name, tweet_vecs)\n",
    "        np.savez(file_name, tweet_vecs)\n",
    "        print \"Saved {} to disk.\".format(name)\n",
    "save_vec(pos_vecs, 'pos')\n",
    "save_vec(neg_vecs, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

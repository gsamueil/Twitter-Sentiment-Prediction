{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os.path\n",
    "DATA_PATH = '/home/sam/Hhd/twitter_sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def function to load data from json to dataframe\n",
    "def load_data(file_name):\n",
    "    print \"Loading: \" + file_name + \" ...\"\n",
    "    data_path = '/home/sam/Hhd/twitter_sentiment/data/'\n",
    "    data_df = pd.read_json(data_path + file_name, lines=True)\n",
    "    # we only take the 'text' column\n",
    "    drop_columns = list(data_df.columns)\n",
    "    drop_columns.remove('text')\n",
    "    data_df.drop(drop_columns, axis = 1, inplace = True)\n",
    "    print \"Done loading json file to dataframe.\"\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: positive.json ...\n",
      "Done loading json file to dataframe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks fam!!! ðŸ˜˜ðŸ˜˜ðŸ˜˜ https://t.co/sbNzT886Vs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @PinGDP: Right back at you Ms Cassie :)\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@notonIyou also my kinda coming out as bi :) #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @kevingschmidt: Thank you ktla5news for hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @watchdogsgame: Celebrate our DedSec member...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0          Thanks fam!!! ðŸ˜˜ðŸ˜˜ðŸ˜˜ https://t.co/sbNzT886Vs\n",
       "1  RT @PinGDP: Right back at you Ms Cassie :)\\n\\n...\n",
       "2  @notonIyou also my kinda coming out as bi :) #...\n",
       "3  RT @kevingschmidt: Thank you ktla5news for hav...\n",
       "4  RT @watchdogsgame: Celebrate our DedSec member..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data('positive.json')\n",
    "df.dropna(axis=0, inplace=True) # drop na rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stopping words:  [u'i', u'me', u'my', u'myself', u'we']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks fam</td>\n",
       "      <td>[thanks, fam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT  Right back at you Ms Cassie \\n\\n</td>\n",
       "      <td>[right, back, ms, cassie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>also my kinda coming out as bi  issagaygirl</td>\n",
       "      <td>[also, kinda, coming, bi, issagaygirl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT  Thank you ktlanews for having  and I on to...</td>\n",
       "      <td>[thank, ktlanews, talk, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT  Celebrate our DedSec members on their birt...</td>\n",
       "      <td>[celebrate, dedsec, members, birthdays, gtgt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>will i see the Jack Johnson followes you in my...</td>\n",
       "      <td>[see, jack, johnson, followes, notifications, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sexy live cam show in  \\nLets get naughty  por...</td>\n",
       "      <td>[sexy, live, cam, show, lets, get, naughty, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>people have decided that Sunbleach is cool en...</td>\n",
       "      <td>[people, decided, sunbleach, cool, enough, fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>My Goddess Marilyn</td>\n",
       "      <td>[goddess, marilyn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Also quite funny to see that outside our droni...</td>\n",
       "      <td>[also, quite, funny, see, outside, dronies, wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                       Thanks fam     \n",
       "1         RT  Right back at you Ms Cassie \\n\\n         \n",
       "2        also my kinda coming out as bi  issagaygirl   \n",
       "3  RT  Thank you ktlanews for having  and I on to...   \n",
       "4  RT  Celebrate our DedSec members on their birt...   \n",
       "5  will i see the Jack Johnson followes you in my...   \n",
       "6  sexy live cam show in  \\nLets get naughty  por...   \n",
       "7   people have decided that Sunbleach is cool en...   \n",
       "8                               My Goddess Marilyn     \n",
       "9  Also quite funny to see that outside our droni...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                                      [thanks, fam]  \n",
       "1                          [right, back, ms, cassie]  \n",
       "2             [also, kinda, coming, bi, issagaygirl]  \n",
       "3                      [thank, ktlanews, talk, link]  \n",
       "4  [celebrate, dedsec, members, birthdays, gtgt, ...  \n",
       "5  [see, jack, johnson, followes, notifications, ...  \n",
       "6  [sexy, live, cam, show, lets, get, naughty, po...  \n",
       "7  [people, decided, sunbleach, cool, enough, fol...  \n",
       "8                                 [goddess, marilyn]  \n",
       "9  [also, quite, funny, see, outside, dronies, wo...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove new line char\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'\\\\n',value=r'')\n",
    "# remove https links\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'(http|https):\\/\\/[^(\\s|\\b)]+',value=r'')\n",
    "# remove user name\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'@\\w+',value=r'')\n",
    "# remove non-alphabet, this includes number and punctuation\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'[^a-zA-Z\\s]',value=r'')\n",
    "# tokenize each tweets to form sentences.\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
    "# remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "add_stop_words = ['amp', 'rt']\n",
    "stop_words += add_stop_words\n",
    "print \"sample stopping words: \", stop_words[:5]\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now let us bring in the wordvec trained using text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-04 11:14:10,126 : INFO : loading Word2Vec object from /home/sam/Hhd/twitter_sentiment/wordvec/tweets.model.bin\n",
      "2017-03-04 11:14:10,128 : INFO : loading wv recursively from /home/sam/Hhd/twitter_sentiment/wordvec/tweets.model.bin.wv.* with mmap=None\n",
      "2017-03-04 11:14:10,128 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-04 11:14:10,128 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-04 11:14:10,129 : INFO : loaded /home/sam/Hhd/twitter_sentiment/wordvec/tweets.model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model tweets.model.bin ...\n",
      "Done loading.\n"
     ]
    }
   ],
   "source": [
    "# check if the model already exists, if so load it else train the model\n",
    "def load_wordvec(sentences, model_name, size = 50):\n",
    "    data_path = '/home/sam/Hhd/twitter_sentiment/'\n",
    "    model_path = data_path + 'wordvec/' + model_name\n",
    "    if os.path.isfile(model_path):\n",
    "        print \"Loading existing model {} ...\".format(model_name)\n",
    "        model = word2vec.Word2Vec.load(model_path)\n",
    "    else:\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        print \"Training using {} ...\".format(model_name)\n",
    "        model = word2vec.Word2Vec(sentences, size=size, sg=1, workers=4)\n",
    "        model.save(model_path)\n",
    "        # If youâ€™re finished training a model (=no more updates, only querying), you can do\n",
    "        model.init_sims(replace=True)\n",
    "    print \"Done loading.\"\n",
    "    return model\n",
    "\n",
    "# sentences = word2vec.Text8Corpus(data_path + 'data/text8')\n",
    "sentences = list(df['tokenized'])\n",
    "vec_size = 50\n",
    "# model = load_wordvec(sentences, 'text8.model.bin')\n",
    "model = load_wordvec(sentences, 'tweets.model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform our tweets using vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length is:  17\n"
     ]
    }
   ],
   "source": [
    "# first find the max length since that decides the padding\n",
    "df['size'] = df['tokenized'].apply(lambda x: len(x))\n",
    "print \"max sentence length is: \", df['size'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting tweets to vec!\n",
      "Total 124 not in vocab.\n"
     ]
    }
   ],
   "source": [
    "# initialize empty arry to fill with vector repsentation\n",
    "n = tweet_tokens.shape[0]\n",
    "m = df['size'].max()\n",
    "n_absent = 0\n",
    "tweet_vecs = np.zeros((n,m,vec_size))\n",
    "vocabs = model.wv.vocab.keys()\n",
    "for i in range(n):\n",
    "    token_i = [x for x in tweet_tokens[i] if x in vocabs]\n",
    "    m_i = len(token_i)\n",
    "    if m_i == 0:\n",
    "        n_absent += 1\n",
    "    else:\n",
    "        diff_i = abs(m_i - m)\n",
    "        vecs_i = model[token_i]\n",
    "        tweet_vecs[i] = np.lib.pad(vecs_i, ((0,diff_i),(0,0)), 'constant', constant_values=0)\n",
    "print \"Done converting tweets to vec!\"\n",
    "print \"Total {} not in vocab.\".format(n_absent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def function to load data from json to dataframe\n",
    "def load_data(file_name):\n",
    "    print \"Loading: \" + file_name + \" ...\"\n",
    "    data_path = './data/'\n",
    "    data_df = pd.read_json(data_path + file_name, lines=True)\n",
    "    # we only take the 'text' column\n",
    "    drop_columns = list(data_df.columns)\n",
    "    drop_columns.remove('text')\n",
    "    data_df.drop(drop_columns, axis = 1, inplace = True)\n",
    "    print \"Done loading json file to dataframe.\"\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: positive.json ...\n",
      "Done loading json file to dataframe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks fam!!! ðŸ˜˜ðŸ˜˜ðŸ˜˜ https://t.co/sbNzT886Vs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @PinGDP: Right back at you Ms Cassie :)\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@notonIyou also my kinda coming out as bi :) #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @kevingschmidt: Thank you ktla5news for hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @watchdogsgame: Celebrate our DedSec member...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0          Thanks fam!!! ðŸ˜˜ðŸ˜˜ðŸ˜˜ https://t.co/sbNzT886Vs\n",
       "1  RT @PinGDP: Right back at you Ms Cassie :)\\n\\n...\n",
       "2  @notonIyou also my kinda coming out as bi :) #...\n",
       "3  RT @kevingschmidt: Thank you ktla5news for hav...\n",
       "4  RT @watchdogsgame: Celebrate our DedSec member..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data('positive.json')\n",
    "df.dropna(axis=0, inplace=True) # drop na rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stopping words:  [u'i', u'me', u'my', u'myself', u'we']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks fam</td>\n",
       "      <td>[thanks, fam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT  Right back at you Ms Cassie \\n\\n</td>\n",
       "      <td>[right, back, ms, cassie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>also my kinda coming out as bi  issagaygirl</td>\n",
       "      <td>[also, kinda, coming, bi, issagaygirl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT  Thank you ktlanews for having  and I on to...</td>\n",
       "      <td>[thank, ktlanews, talk, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT  Celebrate our DedSec members on their birt...</td>\n",
       "      <td>[celebrate, dedsec, members, birthdays, gtgt, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                       Thanks fam     \n",
       "1         RT  Right back at you Ms Cassie \\n\\n         \n",
       "2        also my kinda coming out as bi  issagaygirl   \n",
       "3  RT  Thank you ktlanews for having  and I on to...   \n",
       "4  RT  Celebrate our DedSec members on their birt...   \n",
       "\n",
       "                                           tokenized  \n",
       "0                                      [thanks, fam]  \n",
       "1                          [right, back, ms, cassie]  \n",
       "2             [also, kinda, coming, bi, issagaygirl]  \n",
       "3                      [thank, ktlanews, talk, link]  \n",
       "4  [celebrate, dedsec, members, birthdays, gtgt, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove new line char\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'\\\\n',value=r'')\n",
    "# remove https links\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'(http|https):\\/\\/[^(\\s|\\b)]+',value=r'')\n",
    "# remove user name\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'@\\w+',value=r'')\n",
    "# remove non-alphabet, this includes number and punctuation\n",
    "df['text'].replace(regex=True,inplace=True,to_replace=r'[^a-zA-Z\\s]',value=r'')\n",
    "# tokenize each tweets to form sentences.\n",
    "df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
    "# remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "add_stop_words = ['amp', 'rt']\n",
    "stop_words += add_stop_words\n",
    "print \"sample stopping words: \", stop_words[:5]\n",
    "df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now let us bring in the wordvec trained using text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:21:23,331 : INFO : collecting all words and their counts\n",
      "2017-03-02 21:21:23,334 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using Text8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-02 21:21:28,711 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-03-02 21:21:28,711 : INFO : Loading a fresh vocabulary\n",
      "2017-03-02 21:21:29,090 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2017-03-02 21:21:29,090 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2017-03-02 21:21:29,259 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-03-02 21:21:29,279 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-03-02 21:21:29,280 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n",
      "2017-03-02 21:21:29,280 : INFO : estimated required memory for 71290 words and 500 dimensions: 320805000 bytes\n",
      "2017-03-02 21:21:29,528 : INFO : resetting layer weights\n",
      "2017-03-02 21:21:30,594 : INFO : training model with 4 workers on 71290 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-03-02 21:21:30,594 : INFO : expecting 1701 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-02 21:21:31,615 : INFO : PROGRESS: at 1.20% examples, 733310 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:21:32,619 : INFO : PROGRESS: at 2.42% examples, 743507 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:21:33,622 : INFO : PROGRESS: at 3.52% examples, 722915 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:34,634 : INFO : PROGRESS: at 4.73% examples, 730090 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:35,645 : INFO : PROGRESS: at 5.89% examples, 730073 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:36,641 : INFO : PROGRESS: at 7.11% examples, 737296 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:37,666 : INFO : PROGRESS: at 8.21% examples, 727293 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:38,674 : INFO : PROGRESS: at 9.32% examples, 723478 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:39,677 : INFO : PROGRESS: at 10.37% examples, 715789 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:40,682 : INFO : PROGRESS: at 11.48% examples, 713415 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:41,683 : INFO : PROGRESS: at 12.58% examples, 711556 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:42,685 : INFO : PROGRESS: at 13.80% examples, 716026 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:43,689 : INFO : PROGRESS: at 14.93% examples, 715274 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:44,689 : INFO : PROGRESS: at 16.11% examples, 715416 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:45,702 : INFO : PROGRESS: at 17.34% examples, 718841 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:46,709 : INFO : PROGRESS: at 18.53% examples, 719466 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:47,711 : INFO : PROGRESS: at 19.74% examples, 721267 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:48,723 : INFO : PROGRESS: at 21.01% examples, 724753 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:49,738 : INFO : PROGRESS: at 22.21% examples, 724976 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:50,743 : INFO : PROGRESS: at 23.48% examples, 728273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:51,740 : INFO : PROGRESS: at 24.67% examples, 729062 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:52,758 : INFO : PROGRESS: at 25.84% examples, 729093 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:21:53,767 : INFO : PROGRESS: at 26.96% examples, 728101 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:21:54,764 : INFO : PROGRESS: at 28.18% examples, 729632 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:55,763 : INFO : PROGRESS: at 29.37% examples, 730278 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:56,773 : INFO : PROGRESS: at 30.64% examples, 732568 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:57,779 : INFO : PROGRESS: at 31.89% examples, 734439 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:21:58,782 : INFO : PROGRESS: at 33.02% examples, 733245 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:21:59,793 : INFO : PROGRESS: at 34.25% examples, 734623 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:22:00,786 : INFO : PROGRESS: at 35.38% examples, 733228 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:01,802 : INFO : PROGRESS: at 36.47% examples, 731093 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:22:02,817 : INFO : PROGRESS: at 37.60% examples, 729932 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:03,821 : INFO : PROGRESS: at 38.72% examples, 728821 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:04,831 : INFO : PROGRESS: at 39.80% examples, 726965 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:05,841 : INFO : PROGRESS: at 40.91% examples, 725722 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:06,843 : INFO : PROGRESS: at 42.07% examples, 725449 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:07,855 : INFO : PROGRESS: at 43.21% examples, 724845 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:08,867 : INFO : PROGRESS: at 44.34% examples, 724139 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:09,878 : INFO : PROGRESS: at 45.46% examples, 723430 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:10,892 : INFO : PROGRESS: at 46.55% examples, 722442 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:11,902 : INFO : PROGRESS: at 47.76% examples, 723209 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:22:12,921 : INFO : PROGRESS: at 48.91% examples, 722875 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:13,923 : INFO : PROGRESS: at 50.06% examples, 722884 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:14,928 : INFO : PROGRESS: at 51.24% examples, 723186 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:15,932 : INFO : PROGRESS: at 52.36% examples, 722583 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:16,936 : INFO : PROGRESS: at 53.63% examples, 724107 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:17,939 : INFO : PROGRESS: at 54.70% examples, 722951 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:18,945 : INFO : PROGRESS: at 55.93% examples, 723534 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:19,954 : INFO : PROGRESS: at 57.08% examples, 723296 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:20,956 : INFO : PROGRESS: at 58.27% examples, 723640 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:21,966 : INFO : PROGRESS: at 59.34% examples, 722459 words/s, in_qsize 8, out_qsize 2\n",
      "2017-03-02 21:22:22,974 : INFO : PROGRESS: at 60.44% examples, 721496 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:23,991 : INFO : PROGRESS: at 61.62% examples, 721468 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:22:25,012 : INFO : PROGRESS: at 62.75% examples, 720860 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:26,012 : INFO : PROGRESS: at 63.89% examples, 720698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:27,015 : INFO : PROGRESS: at 64.97% examples, 719922 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-02 21:22:28,029 : INFO : PROGRESS: at 66.07% examples, 719351 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:29,029 : INFO : PROGRESS: at 67.27% examples, 719968 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:30,044 : INFO : PROGRESS: at 68.29% examples, 718569 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:22:31,042 : INFO : PROGRESS: at 69.37% examples, 717824 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:32,064 : INFO : PROGRESS: at 70.57% examples, 718140 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:33,066 : INFO : PROGRESS: at 71.75% examples, 718428 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:34,076 : INFO : PROGRESS: at 72.86% examples, 718037 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:35,085 : INFO : PROGRESS: at 74.03% examples, 718119 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:22:36,087 : INFO : PROGRESS: at 75.12% examples, 717486 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:37,092 : INFO : PROGRESS: at 76.36% examples, 718090 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:38,095 : INFO : PROGRESS: at 77.47% examples, 717759 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:39,115 : INFO : PROGRESS: at 78.65% examples, 717843 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:22:40,118 : INFO : PROGRESS: at 79.75% examples, 717289 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:41,119 : INFO : PROGRESS: at 80.94% examples, 717632 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:42,124 : INFO : PROGRESS: at 82.01% examples, 716766 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:43,139 : INFO : PROGRESS: at 83.19% examples, 716843 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:44,143 : INFO : PROGRESS: at 84.34% examples, 716861 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:45,149 : INFO : PROGRESS: at 85.50% examples, 717082 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:46,152 : INFO : PROGRESS: at 86.70% examples, 717609 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:22:47,166 : INFO : PROGRESS: at 87.92% examples, 718135 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:48,177 : INFO : PROGRESS: at 89.09% examples, 718254 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:49,171 : INFO : PROGRESS: at 90.22% examples, 718129 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:50,172 : INFO : PROGRESS: at 91.31% examples, 717750 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:22:51,187 : INFO : PROGRESS: at 92.43% examples, 717405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:52,197 : INFO : PROGRESS: at 93.52% examples, 716896 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-02 21:22:53,203 : INFO : PROGRESS: at 94.58% examples, 716225 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:54,208 : INFO : PROGRESS: at 95.71% examples, 715872 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:55,238 : INFO : PROGRESS: at 96.84% examples, 715482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-02 21:22:56,251 : INFO : PROGRESS: at 98.02% examples, 715699 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-02 21:22:57,274 : INFO : PROGRESS: at 99.17% examples, 715512 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-02 21:22:57,979 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-02 21:22:57,985 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-02 21:22:57,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-02 21:22:57,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-02 21:22:57,996 : INFO : training on 85026035 raw words (62527570 effective words) took 87.4s, 715423 effective words/s\n",
      "2017-03-02 21:22:57,996 : INFO : saving Word2Vec object under text.model.bin, separately None\n",
      "2017-03-02 21:22:57,997 : INFO : not storing attribute syn0norm\n",
      "2017-03-02 21:22:57,998 : INFO : storing np array 'syn0' to text.model.bin.wv.syn0.npy\n",
      "2017-03-02 21:22:58,067 : INFO : storing np array 'syn1neg' to text.model.bin.syn1neg.npy\n",
      "2017-03-02 21:22:58,136 : INFO : not storing attribute cum_table\n",
      "2017-03-02 21:22:58,829 : INFO : saved text.model.bin\n",
      "2017-03-02 21:22:58,830 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# check if the model already exists, if so load it else train the model\n",
    "def load_wordvec():\n",
    "    if os.path.isfile('./wordvec/text.model.bin'):\n",
    "        print \"Loading existing model ...\"\n",
    "        model = word2vec.Word2Vec.load('./wordvec/text.model.bin')\n",
    "    else:\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        sentences = word2vec.Text8Corpus('./data/text8')\n",
    "        print \"Training using Text8 ...\"\n",
    "        model = word2vec.Word2Vec(sentences, size=500, workers=4)\n",
    "        model.save('text.model.bin')\n",
    "        # If youâ€™re finished training a model (=no more updates, only querying), you can do\n",
    "        model.init_sims(replace=True)\n",
    "    print \"Done loading.\"\n",
    "    return model\n",
    "model = load_wordvec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'queen', 0.625677227973938)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick look at the model\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

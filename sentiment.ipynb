{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os.path\n",
    "# FILE_PATH = '/home/sam/Hhd/twitter_sentiment/'\n",
    "FILE_PATH = '/home/sam/Data/twitter_sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def function to load data from json to dataframe\n",
    "def load_data(file_name):\n",
    "    print \"Loading: \" + file_name + \" ...\"\n",
    "    data_path = FILE_PATH + 'data/'\n",
    "    data_df = pd.read_json(data_path + file_name, lines=True)\n",
    "    # we only take the 'text' column\n",
    "    drop_columns = list(data_df.columns)\n",
    "    drop_columns.remove('text')\n",
    "    data_df.drop(drop_columns, axis = 1, inplace = True)\n",
    "    print \"Done loading json file to dataframe.\"\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: positive.json ...\n",
      "Done loading json file to dataframe.\n",
      "                                                text\n",
      "0          Thanks fam!!! ðŸ˜˜ðŸ˜˜ðŸ˜˜ https://t.co/sbNzT886Vs\n",
      "1  RT @PinGDP: Right back at you Ms Cassie :)\\n\\n...\n",
      "2  @notonIyou also my kinda coming out as bi :) #...\n",
      "3  RT @kevingschmidt: Thank you ktla5news for hav...\n",
      "4  RT @watchdogsgame: Celebrate our DedSec member...\n",
      "Loading: negative.json ...\n",
      "Done loading json file to dataframe.\n",
      "                                                text\n",
      "0                 I loved it https://t.co/nhezqTqCcc\n",
      "1                     I want to get my nails done :(\n",
      "2  RT @BucamanWWE: This is the last were seeing t...\n",
      "3  @TheDauntingFray // Lucky I've never done that...\n",
      "4                 @JonahBonahh I love her so much :(\n"
     ]
    }
   ],
   "source": [
    "df_pos = load_data('positive.json')\n",
    "df_pos.dropna(axis=0, inplace=True) # drop na rows\n",
    "print df_pos.head()\n",
    "df_neg = load_data('negative.json')\n",
    "df_neg.dropna(axis=0, inplace=True) # drop na rows\n",
    "print df_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stopping words:  [u'i', u'me', u'my', u'myself', u'we']\n",
      "                                                text  \\\n",
      "0                                       Thanks fam     \n",
      "1         RT  Right back at you Ms Cassie \\n\\n         \n",
      "2        also my kinda coming out as bi  issagaygirl   \n",
      "3  RT  Thank you ktlanews for having  and I on to...   \n",
      "4  RT  Celebrate our DedSec members on their birt...   \n",
      "\n",
      "                                           tokenized  \n",
      "0                                      [thanks, fam]  \n",
      "1                          [right, back, ms, cassie]  \n",
      "2             [also, kinda, coming, bi, issagaygirl]  \n",
      "3                      [thank, ktlanews, talk, link]  \n",
      "4  [celebrate, dedsec, members, birthdays, gtgt, ...  \n",
      "sample stopping words:  [u'i', u'me', u'my', u'myself', u'we']\n",
      "                                                text  \\\n",
      "0                                        I loved it    \n",
      "1                       I want to get my nails done    \n",
      "2  RT  This is the last were seeing the Hardys fo...   \n",
      "3                  Lucky Ive never done that before    \n",
      "4                                I love her so much    \n",
      "\n",
      "                                     tokenized  \n",
      "0                                      [loved]  \n",
      "1                     [want, get, nails, done]  \n",
      "2  [last, seeing, hardys, minute, impactonpop]  \n",
      "3                    [lucky, ive, never, done]  \n",
      "4                                 [love, much]  \n"
     ]
    }
   ],
   "source": [
    "def pre_process(df):\n",
    "    # remove new line char\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'\\\\n',value=r'')\n",
    "    # remove https links\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'(http|https):\\/\\/[^(\\s|\\b)]+',value=r'')\n",
    "    # remove user name\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'@\\w+',value=r'')\n",
    "    # remove non-alphabet, this includes number and punctuation\n",
    "    df['text'].replace(regex=True,inplace=True,to_replace=r'[^a-zA-Z\\s]',value=r'')\n",
    "    # tokenize each tweets to form sentences.\n",
    "    df['tokenized'] = df.apply(lambda row: nltk.word_tokenize(row['text'].lower()), axis=1)\n",
    "    # remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    add_stop_words = ['amp', 'rt']\n",
    "    stop_words += add_stop_words\n",
    "    print \"sample stopping words: \", stop_words[:5]\n",
    "    df['tokenized'] = df['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    print df.head(5)\n",
    "#     return df\n",
    "pre_process(df_pos)\n",
    "pre_process(df_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now let us bring in the wordvec trained using text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-04 17:46:30,523 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-03-04 17:46:30,524 : INFO : collecting all words and their counts\n",
      "2017-03-04 17:46:30,526 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-04 17:46:30,545 : INFO : collected 6412 word types from a corpus of 26139 raw words and 4599 sentences\n",
      "2017-03-04 17:46:30,546 : INFO : Loading a fresh vocabulary\n",
      "2017-03-04 17:46:30,554 : INFO : min_count=5 retains 980 unique words (15% of original 6412, drops 5432)\n",
      "2017-03-04 17:46:30,555 : INFO : min_count=5 leaves 18198 word corpus (69% of original 26139, drops 7941)\n",
      "2017-03-04 17:46:30,561 : INFO : deleting the raw counts dictionary of 6412 items\n",
      "2017-03-04 17:46:30,563 : INFO : sample=0.001 downsamples 72 most-common words\n",
      "2017-03-04 17:46:30,563 : INFO : downsampling leaves estimated 15571 word corpus (85.6% of prior 18198)\n",
      "2017-03-04 17:46:30,564 : INFO : estimated required memory for 980 words and 50 dimensions: 882000 bytes\n",
      "2017-03-04 17:46:30,570 : INFO : resetting layer weights\n",
      "2017-03-04 17:46:30,590 : INFO : training model with 4 workers on 980 vocabulary and 50 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-03-04 17:46:30,591 : INFO : expecting 4599 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-04 17:46:30,686 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-04 17:46:30,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-04 17:46:30,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-04 17:46:30,701 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-04 17:46:30,702 : INFO : training on 130695 raw words (77867 effective words) took 0.1s, 719784 effective words/s\n",
      "2017-03-04 17:46:30,702 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-03-04 17:46:30,703 : INFO : saving Word2Vec object under /home/sam/Data/twitter_sentiment/wordvec/tweets.model.bin, separately None\n",
      "2017-03-04 17:46:30,703 : INFO : not storing attribute syn0norm\n",
      "2017-03-04 17:46:30,704 : INFO : not storing attribute cum_table\n",
      "2017-03-04 17:46:30,711 : INFO : saved /home/sam/Data/twitter_sentiment/wordvec/tweets.model.bin\n",
      "2017-03-04 17:46:30,712 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using tweets.model.bin ...\n",
      "Done building.\n"
     ]
    }
   ],
   "source": [
    "# check if the model already exists, if so load it else train the model\n",
    "def build_wordvec(sentences, model_name, size = 50):\n",
    "    model_path = FILE_PATH + 'wordvec/' + model_name\n",
    "    if os.path.isfile(model_path):\n",
    "        print \"Loading existing model {} ...\".format(model_name)\n",
    "        model = word2vec.Word2Vec.load(model_path)\n",
    "    else:\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        print \"Training using {} ...\".format(model_name)\n",
    "        model = word2vec.Word2Vec(sentences, size=size, sg=1, workers=4)\n",
    "        model.save(model_path)\n",
    "        # If youâ€™re finished training a model (=no more updates, only querying), you can do\n",
    "        model.init_sims(replace=True)\n",
    "    print \"Done building.\"\n",
    "    return model\n",
    "\n",
    "# sentences = word2vec.Text8Corpus(data_path + 'data/text8')              # use text 8\n",
    "sentences = list(df_pos['tokenized']) + list(df_neg['tokenized'])         # use just tweets itself\n",
    "vec_size = 50\n",
    "# model = load_wordvec(sentences, 'text8.model.bin')\n",
    "model = build_wordvec(sentences, 'tweets.model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform our tweets using vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first find the max length since that decides the padding\n",
    "def max_len(df):\n",
    "    df['size'] = df['tokenized'].apply(lambda x: len(x))\n",
    "    print \"max sentence length is: \", df['size'].max()\n",
    "    return df['size'].max()\n",
    "max_total = max(max_len(df_pos), max_len(df_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize empty arry to fill with vector repsentation\n",
    "def convert2vec(df, max_total):\n",
    "    tweet_tokens = df['tokenized']\n",
    "    n = tweet_tokens.shape[0]\n",
    "    m = max_total\n",
    "    n_absent = 0\n",
    "    tweet_vecs = np.zeros((n,m,vec_size))\n",
    "    vocabs = model.wv.vocab.keys()\n",
    "    for i in range(n):\n",
    "        token_i = [x for x in tweet_tokens[i] if x in vocabs]\n",
    "        m_i = len(token_i)\n",
    "        if m_i == 0:\n",
    "            n_absent += 1\n",
    "        else:\n",
    "            diff_i = abs(m_i - m)\n",
    "            vecs_i = model[token_i]\n",
    "            tweet_vecs[i] = np.lib.pad(vecs_i, ((0,diff_i),(0,0)), 'constant', constant_values=0)\n",
    "    print \"Done converting tweets to vec!\"\n",
    "    print \"Total {} not in vocab.\".format(n_absent)\n",
    "    return tweet_vecs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tweet_vecs to disk in npy\n",
    "if os.path.isfile(FILE_PATH+'tweet_vecs.npy') and os.path.isfile(FILE_PATH+'tweet_vecs.npz'):\n",
    "    print \"npy already exists.\"\n",
    "else:\n",
    "    np.save(FILE_PATH+'tweet_vecs', tweet_vecs)\n",
    "    np.savez(FILE_PATH+'tweet_vecs', tweet_vecs)\n",
    "    print \"Saved tweet_vecs to disk.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
